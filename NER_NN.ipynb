{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_NN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqsTQT4npxTNizs8qYF63s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushjain1144/NER/blob/master/NER_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6eRcJnO4Xyl",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGT8kFRu4VyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYDqojOAmafp",
        "colab_type": "text"
      },
      "source": [
        "# Loading the features and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRlPGzpimTlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "da8ca8ee-66a7-483b-cd10-a218e269fe69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ln -s content/gdrive/My\\ Drive/NER /ner_dir"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOChUptq2VdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ddfe5ec8-3b5a-4c74-8900-92c83c7013c9"
      },
      "source": [
        "%cd ..\n",
        "%cd /ner_dir\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/content/gdrive/My Drive/NER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvCu2tfk2nVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "56ea8ec1-40b8-4aa7-8cfa-da943666bb1b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "activations.py\t\t\t      params-1.npy\n",
            "Conll.ipynb\t\t\t      params-200.npy\n",
            "dataset\t\t\t\t      params-250.npy\n",
            "initial_experiment.ipynb\t      params-2.npy\n",
            "initialization.py\t\t      params-300.npy\n",
            "loss.py\t\t\t\t      params-50.npy\n",
            "NER_NN.ipynb\t\t\t      params-50-random_initialization.npy\n",
            "NER_NN_network.ipynb\t\t      params-50-range_initialization.npy\n",
            "nn.py\t\t\t\t      __pycache__\n",
            "params-0.npy\t\t\t      README.md\n",
            "params-0-random_initialization.npy    test_features.npy\n",
            "params-0-range_initialization.npy     train_features.npy\n",
            "params-100.npy\t\t\t      val_features.npy\n",
            "params-100-random_initialization.npy  vocab.npy\n",
            "params-100-range_initialization.npy   ytest.npy\n",
            "params-150.npy\t\t\t      ytrain.npy\n",
            "params-150-range_initialization.npy   yval.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbtd6U_5-wDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import activations\n",
        "import loss\n",
        "import initialization as init_layer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkgIekd-4RmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = np.load('vocab.npy')\n",
        "train_features = np.load('train_features.npy').T\n",
        "test_features = np.load('test_features.npy').T\n",
        "val_features = np.load('val_features.npy').T\n",
        "y_train = np.load('ytrain.npy').T\n",
        "y_val = np.load('yval.npy').T\n",
        "y_test = np.load('ytest.npy').T"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N02tcTD9H5cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "3dfd7ddc-da53-48fb-d2f7-118f7dfd7f09"
      },
      "source": [
        "print(train_features.shape)\n",
        "print(vocab.shape)\n",
        "print(test_features.shape)\n",
        "print(val_features.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 204566)\n",
            "(26872, 300)\n",
            "(900, 46665)\n",
            "(900, 51577)\n",
            "(10, 204566)\n",
            "(10, 46665)\n",
            "(10, 51577)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftzW0XaYlGxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "5cc9ece0-8ce9-4935-b1ca-1f0a4f62df5f"
      },
      "source": [
        "train_features[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        , -0.92605459,  0.00982666, ...,  1.38121068,\n",
              "         0.05078125,  1.46252757],\n",
              "       [ 0.        , -1.13792351,  0.2265625 , ...,  0.8632994 ,\n",
              "        -0.09326172, -0.07399186],\n",
              "       [ 0.        , -0.7880129 ,  0.28125   , ...,  0.76144395,\n",
              "         0.06494141,  0.03143081],\n",
              "       ...,\n",
              "       [ 0.        , -0.43820235, -0.03540039, ...,  0.7176954 ,\n",
              "        -0.08154297,  0.11263644],\n",
              "       [ 0.        , -0.95179252,  0.14746094, ...,  0.69231712,\n",
              "         0.13085938,  0.80112245],\n",
              "       [ 0.        , -1.45894089,  0.12890625, ..., -1.61798501,\n",
              "         0.12597656, -0.11506672]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M91iJFgGrBwk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "2f2c5c18-4f10-43a4-a097-8dfb3481b01a"
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 1., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEzSQKPHJv7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = 1\n",
        "word_vector_dim = 300\n",
        "num_tags = 10"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K1vS8CzG0W0",
        "colab_type": "text"
      },
      "source": [
        "# Neural Netwwork Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fStbOrS4yW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_architecture = [\n",
        "  {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "  {\"layer_size\": 300, \"activation\": \"relu\"},\n",
        "  {\"layer_size\": 100, \"activation\": \"relu\"},\n",
        "  {\"layer_size\": 10, \"activation\": \"none\"}\n",
        "]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHYrA8lQmW-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c487a40-0e2a-46ec-81b5-51d64b2f7e6c"
      },
      "source": [
        "nn_architecture[1]['activation']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apFzoggrMYW_",
        "colab_type": "text"
      },
      "source": [
        "# Initialize Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FNVIQaL_5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(nn_architecture, initialization = \"range_initialization\", seed=5):\n",
        "\n",
        "  parameters = {}\n",
        "  num_layers = len(nn_architecture)\n",
        "\n",
        "  for i in range(1, num_layers):\n",
        "\n",
        "    if initialization == \"range_initialization\":\n",
        "      parameters['W' + str(i)] = init_layer.range_initializtion(nn_architecture[i][\"layer_size\"],\n",
        "                                            nn_architecture[i - 1][\"layer_size\"], seed)\n",
        "    else:\n",
        "      parameters['W' + str(i)] = init_layer.random_initialization(nn_architecture[i][\"layer_size\"],\n",
        "                                            nn_architecture[i - 1][\"layer_size\"], seed)\n",
        "\n",
        "    parameters['b' + str(i)] = np.zeros((nn_architecture[i][\"layer_size\"], 1))\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk0yEPOsggOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "5a1e4769-1535-404c-fdb7-b9fa839a1968"
      },
      "source": [
        "param = initialize_parameters(nn_architecture, initialization=\"range_initialization\")\n",
        "for k in param.keys():\n",
        "  print(f\"{k}: {param[k].shape}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1: (300, 900)\n",
            "b1: (300, 1)\n",
            "W2: (100, 300)\n",
            "b2: (100, 1)\n",
            "W3: (10, 100)\n",
            "b3: (10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RojXQmNkcTjY",
        "colab_type": "text"
      },
      "source": [
        "# Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg5s5MaUQKYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Z = W * X + b\n",
        "# Here A is output of previous layer\n",
        "\n",
        "def linear_forward(A_prev, W, b):\n",
        "  return np.dot(W, A_prev) + b\n",
        "\n",
        "# apply activation h:  A = h(X) \n",
        "def apply_activation(A, activation, alpha=0.01):\n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return activations.sigmoid(A)\n",
        "  elif activation == \"tanh\":\n",
        "    return activations.tanh(A)\n",
        "  elif activation == \"relu\":\n",
        "    return activations.relu(A)\n",
        "  elif activation == \"leaky_relu\":\n",
        "    return activations.leaky_relu(A, alpha)\n",
        "  elif activation == 'none':\n",
        "    #print(\"None activation used\")\n",
        "    return A\n",
        "  else:\n",
        "    print(f\"ERROR: {activation} activation not supported\")\n",
        "    sys.exit(1)\n",
        "  \n",
        "# driver forward propogation\n",
        "def model_forward(X, parameters, nn_architecture, alpha=0.01):\n",
        "\n",
        "  forward_cache = []\n",
        "  A = X\n",
        "  num_layers = len(nn_architecture)\n",
        "\n",
        "  for i in range(1, num_layers):\n",
        "    A_prev = A\n",
        "    print(f\"{A_prev.shape}: for\")\n",
        "    W = parameters[\"W\" + str(i)]\n",
        "    b = parameters[\"b\" + str(i)]\n",
        "\n",
        "    Z = linear_forward(A_prev, W, b)\n",
        "    activation = nn_architecture[i]['activation']\n",
        "    A = apply_activation(Z, activation, alpha)\n",
        "\n",
        "    forward_cache.append(((A_prev, W, b), Z))\n",
        "\n",
        "  return A, forward_cache  "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuiILZ1FMuJA",
        "colab_type": "text"
      },
      "source": [
        "# Backpropogation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaNyla5vMtrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dz, cache):\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1]\n",
        "\n",
        "  dw = (1 / m) * np.dot(dz, A_prev.T)\n",
        "  db = (1 / m) * np.sum(dz, axis=1, keepdims=True)\n",
        "  dA_prev = np.dot(W.T, dz)\n",
        "\n",
        "  assert dA_prev.shape == A_prev.shape\n",
        "  assert dw.shape == W.shape\n",
        "  assert db.shape == b.shape\n",
        "\n",
        "  return dA_prev, dw, db      "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSHajBeXr9fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_activation_backward(dA, cache, activation_fn):\n",
        "  linear_cache, activation_cache = cache\n",
        "\n",
        "  if activation_fn == \"sigmoid\":\n",
        "    dZ = activations.sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation_fn == \"tanh\":\n",
        "    dZ = activations.sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation_fn == \"relu\":\n",
        "    dZ = activations.sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation_fn == \"leaky_relu\":\n",
        "    dZ = activations.sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "  \n",
        "  elif activation_fn == \"none\":\n",
        "    dZ = dA\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "  \n",
        "  else:\n",
        "    print(\"Activation not available\")\n",
        "    sys.exit(1)\n",
        "\n",
        "  return dA_prev, dw, db\n",
        "\n",
        "  \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKQVgsCqetdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_backward(AL, y, caches, nn_architecture):\n",
        "   \n",
        "  y = y.reshape(AL.shape)\n",
        "  L = len(caches)\n",
        "  grads = {}\n",
        "\n",
        "  dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
        "  # print(caches.keys())\n",
        "  # print(grads.keys())\n",
        "  # print(len(caches))\n",
        "  grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = apply_activation_backward(dAL, caches[L-1], nn_architecture[L-1]['activation'])\n",
        "\n",
        "  for l in range(L - 1, 0, -1):\n",
        "    current_cache = caches[l - 1]\n",
        "    grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)],  \\\n",
        "        grads[\"db\" + str(l)] = apply_activation_backward(\n",
        "            grads[\"dA\" + str(l)], current_cache, \n",
        "            nn_architecture[l]['activation']\n",
        "        )\n",
        "  return grads"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIGxsD-84Nhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, lr):\n",
        "\n",
        "  L = len(parameters) // 2\n",
        "\n",
        "  for l in range(1, L + 1):\n",
        "    parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - \\\n",
        "            lr * grads[\"dW\" + str(l)]\n",
        "    parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - \\\n",
        "            lr * grads[\"db\" + str(l)]\n",
        "  return parameters"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sl_Wp-H5otj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X, y, nn_architecture, initialization='range_initialisation', lr=0.01, num_iterations=2000, print_cost=True, checkpoint_initialisation=None):\n",
        "\n",
        "  np.random.seed(1)\n",
        "\n",
        "  if checkpoint_initialisation == None:\n",
        "    parameters = initialize_parameters(nn_architecture, initialization)\n",
        "  else:\n",
        "    print(f\"Loading checkpoints from file {checkpoint_initialisation}\")\n",
        "    parameters = np.load(checkpoint_initialisation, allow_pickle=True).item()\n",
        "\n",
        "  cost_list = []\n",
        "  val_list = []\n",
        "  #iterate over iterations\n",
        "\n",
        "  for i in tqdm_notebook(range(0, num_iterations)):\n",
        "\n",
        "    #forward step\n",
        "    AL, caches = model_forward(X, parameters, nn_architecture)\n",
        "\n",
        "    cost = loss.cross_entropy_loss(AL, y)\n",
        "\n",
        "    grads = model_backward(AL, y, caches, nn_architecture)\n",
        "\n",
        "    parameters = update_parameters(parameters, grads, lr)\n",
        "\n",
        "    if (i + 1) % 10 == 0 and print_cost:\n",
        "      print(f\"The cost after {i + 1} iterations is: {cost: .4f}\")\n",
        "      acc = accuracy(val_features, parameters, y_val, nn_architecture)\n",
        "      print(f\"The val accuracy after {i + 1} iterations is: {acc}\")\n",
        "      val_list.append(acc)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      cost_list.append(cost)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(f\"Creating checkpoint for {i}th iteration\")\n",
        "      np.save(f'params-{i}-{initialization}.npy', parameters)\n",
        "\n",
        "    #plotting cost curve\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(cost_list)\n",
        "  plt.xlabel(\"Iterations (per 50)\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(f\"Loss curve for the learning rate = {lr}\")\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(val_list)\n",
        "  plt.xlabel(\"Iterations (per 50)\")\n",
        "  plt.ylabel(\"Val Accuracy\")\n",
        "  plt.title(f\"Val Accuracy curve for the learning rate = {lr}\")\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM0ETWnl0-yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(X, parameters, y, nn_architecture):\n",
        "  probs, caches = model_forward(X, parameters, nn_architecture)\n",
        "  labels = (probs >= 0.5) * 1\n",
        "  accuracy = np.mean(labels == y) * 100\n",
        "\n",
        "  return f\"The accuracy rate is {accuracy: .2f}%.\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5mkOLeD1r0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# params = model(train_features, y_train, nn_architecture, initialization='range_initialization', lr=0.01, num_iterations=300)\n",
        "\n",
        "# accuracy(test_features, params, y_test, nn_architecture)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Y6qD9v2sSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Todo\n",
        "\n",
        "# Implement gradient check\n",
        "#Implement precision and recall\n",
        "#implement margin loss\n",
        "#implement regularisation"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wwpwhANGS1A",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31iXipAcF1jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "def dictionary_to_vector(params_dict):\n",
        "  count = 0\n",
        "\n",
        "  for key in params_dict.keys():\n",
        "    new_vector = np.reshape(params_dict[key], (-1, 1))\n",
        "\n",
        "    if count == 0:\n",
        "      theta_vector = new_vector\n",
        "    else:\n",
        "      theta_vector = np.concatenate((theta_vector, new_vector))\n",
        "    count += 1\n",
        "\n",
        "  return theta_vector\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vInG5mpFM_Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vector_to_dictionary(vector, nn_architecture):\n",
        "\n",
        "  L = len(nn_architecture)\n",
        "  parameters = {}\n",
        "  k = 0\n",
        "\n",
        "  for l in range(1, L):\n",
        "\n",
        "    w_dim = nn_architecture[l]['layer_size'] * nn_architecture[l - 1]['layer_size']\n",
        "    b_dim = nn_architecture[l]['layer_size']\n",
        "\n",
        "    temp_dim = k + w_dim\n",
        "\n",
        "    parameters[\"W\" + str(l)] = vector[k:temp_dim].reshape(nn_architecture[l]['layer_size'], nn_architecture[l - 1]['layer_size'])\n",
        "    parameters[\"b\" + str(l)] = vector[temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n",
        "\n",
        "    k += w_dim + b_dim\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTMNkz1qOQM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradients_to_vector(gradients):\n",
        "\n",
        "  valid_grads = [key for key in gradients.keys() if not key.startswith(\"dA\")]\n",
        "  L = len(valid_grads) // 2\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  for l in range(1, L + 1):\n",
        "    if count == 0:\n",
        "      new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n",
        "      new_grads = np.concatenate((new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
        "\n",
        "    else:\n",
        "      new_grads = np.concatenate((new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)))\n",
        "      new_grads = np.concatenate((new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
        "\n",
        "    count += 1\n",
        "  return new_grads"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuTgjZ6vHo7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop_cost(X, parameters, Y, nn_architecture):\n",
        "\n",
        "  AL, _ = model_forward(X, parameters, nn_architecture)\n",
        "  cost = loss.cross_entropy_loss(AL, Y)\n",
        "  return cost\n",
        "\n",
        "def gradient_check(parameters, gradients, X, Y, nn_architecture, epsilon=1e-7):\n",
        "\n",
        "  parameters_vector = dictionary_to_vector(parameters)\n",
        "  gradients_vector = gradients_to_vector(gradients)\n",
        "\n",
        "  grads_approx = np.zeros_like(parameters_vector)\n",
        "\n",
        "  for i in range(len(parameters_vector)):\n",
        "\n",
        "    theta_plus = np.copy(parameters_vector)\n",
        "    theta_plus[i] = theta_plus[i] + epsilon\n",
        "    j_plus = forward_prop_cost(X, vector_to_dictionary(theta_plus, nn_architecture), Y, nn_architecture)\n",
        "\n",
        "\n",
        "    theta_minus = np.copy(parameters_vector)\n",
        "    theta_minus[i] = theta_minus[i] - epsilon\n",
        "    j_minus = forward_prop_cost(X, vector_to_dictionary(theta_minus, nn_architecture), Y, nn_architecture)\n",
        "\n",
        "    grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n",
        "\n",
        "  \n",
        "\n",
        "  numerator = norm(gradients_vector - grads_approx)\n",
        "  denominator = norm(grads_approx) + norm(gradients_vector)\n",
        "\n",
        "  difference = numerator / denominator\n",
        "\n",
        "  if difference > 10e-7:\n",
        "    print(f\"Backprop Wrong: difference = {difference}\")\n",
        "  else:\n",
        "    print(f\"Backprop Correct: difference = {difference}\")\n",
        "\n",
        "  return difference\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WH5lM5MRgu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2b1574ab-11ae-42be-80e9-a1202f7e1a1e"
      },
      "source": [
        "# performing graident checking\n",
        "\n",
        "parameters = initialize_parameters(nn_architecture)\n",
        "\n",
        "perms = np.random.permutation(train_features.shape[1])\n",
        "index = perms[:1]\n",
        "print(train_features[:, index].shape)\n",
        "print(y_train[:, index].shape)\n",
        "\n",
        "AL, caches = model_forward(train_features[:, index], parameters, nn_architecture)\n",
        "\n",
        "grads = model_backward(AL, y_train[:, index], caches, nn_architecture)\n",
        "\n",
        "difference = gradient_check(parameters, grads, train_features[:, index], y_train[:, index], nn_architecture)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 1)\n",
            "(10, 1)\n",
            "Backprop Wrong: difference = 0.8625335372522391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1c-ng2iTemB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_statistics(y, y_pred):\n",
        "\n",
        "  tp = sum()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}