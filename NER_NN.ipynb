{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_NN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO33ZeE34HKDtmwcadYpBg/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b60999c5a1bf4ed98bd9b9399abed60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8cda148d6eac47fb91ad69a01f990a0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1ed8eeb4361542d08bc47cba70241c84",
              "IPY_MODEL_c265cae3ed924354822d728902c0b5c9"
            ]
          }
        },
        "8cda148d6eac47fb91ad69a01f990a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ed8eeb4361542d08bc47cba70241c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_94bf97fbfa264eb389d4ad7f2745c2d9",
            "_dom_classes": [],
            "description": " 24%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 300,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 72,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b835ee03561458687661cb250f2cbde"
          }
        },
        "c265cae3ed924354822d728902c0b5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a8df44f379a45b3962a28cc66666a37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 72/300 [26:59&lt;1:24:38, 22.27s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6daaf879d1c34c919ce4aac181dab264"
          }
        },
        "94bf97fbfa264eb389d4ad7f2745c2d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b835ee03561458687661cb250f2cbde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a8df44f379a45b3962a28cc66666a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6daaf879d1c34c919ce4aac181dab264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushjain1144/NER/blob/master/NER_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6eRcJnO4Xyl",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGT8kFRu4VyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "6aa890a1-5a0d-4d3a-a58e-2bfbca6eea8d"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYDqojOAmafp",
        "colab_type": "text"
      },
      "source": [
        "# Loading the features and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRlPGzpimTlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "6def93d4-e368-4e62-db9e-7a665f18ae63"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ln -s content/gdrive/My\\ Drive/NER /ner_dir"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOChUptq2VdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "63932056-a980-45be-af1d-214d2c9cd8dd"
      },
      "source": [
        "%cd ..\n",
        "%cd /ner_dir\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/content/gdrive/My Drive/NER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvCu2tfk2nVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "acdf0a24-d0b9-43d0-d5c1-124ead3dcbc7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "activations.py\t\t\t      params-50.npy\n",
            "cal_statistics.py\t\t      params-50-random_initialization.npy\n",
            "Conll.ipynb\t\t\t      params-50-range_initialization.npy\n",
            "dataset\t\t\t\t      params-550-range_initialization.npy\n",
            "fig1-loss-Exp1.jpg\t\t      params-600-range_initialization.npy\n",
            "fig1-loss-Exp2.jpg\t\t      params-650-range_initialization.npy\n",
            "fig1-loss-Exp3.jpg\t\t      params-700-range_initialization.npy\n",
            "fig1-loss-Exp4.jpg\t\t      params-750-range_initialization.npy\n",
            "fig1-loss-Exp5.jpg\t\t      params-800-range_initialization.npy\n",
            "initial_experiment.ipynb\t      params-850-range_initialization.npy\n",
            "initialization.py\t\t      params-900-range_initialization.npy\n",
            "loss.py\t\t\t\t      params-950-range_initialization.npy\n",
            "NER_NN.ipynb\t\t\t      __pycache__\n",
            "NER_NN_network.ipynb\t\t      README.md\n",
            "nn.py\t\t\t\t      results_bal.txt\n",
            "params-0.npy\t\t\t      results.txt\n",
            "params-0-random_initialization.npy    statistics.py\n",
            "params-0-range_initialization.npy     test_features.npy\n",
            "params-100.npy\t\t\t      train_features.npy\n",
            "params-100-random_initialization.npy  train-loss-Exp1.jpg\n",
            "params-100-range_initialization.npy   train-loss-Exp2.jpg\n",
            "params-150.npy\t\t\t      train-loss-Exp3.jpg\n",
            "params-150-range_initialization.npy   train-loss-Exp4.jpg\n",
            "params-1.npy\t\t\t      train-loss-Exp5.jpg\n",
            "params-200.npy\t\t\t      val-acc-Exp1.jpg\n",
            "params-200-range_initialization.npy   val-acc-Exp2.jpg\n",
            "params-250.npy\t\t\t      val-acc-Exp3.jpg\n",
            "params-250-range_initialization.npy   val-acc-Exp4.jpg\n",
            "params-2.npy\t\t\t      val-acc-Exp5.jpg\n",
            "params-300.npy\t\t\t      val_features.npy\n",
            "params-300-range_initialization.npy   vocab.npy\n",
            "params-350-range_initialization.npy   ytest.npy\n",
            "params-400-range_initialization.npy   ytrain.npy\n",
            "params-450-range_initialization.npy   yval.npy\n",
            "params-500-range_initialization.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbtd6U_5-wDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import activations\n",
        "import loss\n",
        "import initialization as init_layer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkgIekd-4RmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = np.load('vocab.npy')\n",
        "train_features = np.load('train_features.npy').T\n",
        "test_features = np.load('test_features.npy').T\n",
        "val_features = np.load('val_features.npy').T\n",
        "y_train = np.load('ytrain.npy').T\n",
        "y_val = np.load('yval.npy').T\n",
        "y_test = np.load('ytest.npy').T"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N02tcTD9H5cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "7419ae43-92cc-4e49-9d4d-b9d313e1e73b"
      },
      "source": [
        "print(train_features.shape)\n",
        "print(vocab.shape)\n",
        "print(test_features.shape)\n",
        "print(val_features.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 204566)\n",
            "(26872, 300)\n",
            "(900, 46665)\n",
            "(900, 51577)\n",
            "(10, 204566)\n",
            "(10, 46665)\n",
            "(10, 51577)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftzW0XaYlGxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "4321b012-cfc9-4d8b-c773-b622e2aab4b4"
      },
      "source": [
        "train_features[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        , -0.92605459,  0.00982666, ...,  1.38121068,\n",
              "         0.05078125,  1.46252757],\n",
              "       [ 0.        , -1.13792351,  0.2265625 , ...,  0.8632994 ,\n",
              "        -0.09326172, -0.07399186],\n",
              "       [ 0.        , -0.7880129 ,  0.28125   , ...,  0.76144395,\n",
              "         0.06494141,  0.03143081],\n",
              "       ...,\n",
              "       [ 0.        , -0.43820235, -0.03540039, ...,  0.7176954 ,\n",
              "        -0.08154297,  0.11263644],\n",
              "       [ 0.        , -0.95179252,  0.14746094, ...,  0.69231712,\n",
              "         0.13085938,  0.80112245],\n",
              "       [ 0.        , -1.45894089,  0.12890625, ..., -1.61798501,\n",
              "         0.12597656, -0.11506672]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M91iJFgGrBwk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "2ea230a1-58da-4b3c-cc54-06d02e7b20e6"
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 1., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEzSQKPHJv7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = 1\n",
        "word_vector_dim = 300\n",
        "num_tags = 10"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K1vS8CzG0W0",
        "colab_type": "text"
      },
      "source": [
        "# Neural Netwwork Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fStbOrS4yW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_architecture = [\n",
        "  {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "  #{\"layer_size\": 300, \"activation\": \"relu\"},\n",
        "  #{\"layer_size\": 100, \"activation\": \"relu\"},\n",
        "  {\"layer_size\": 10, \"activation\": \"sigmoid\"}\n",
        "]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHYrA8lQmW-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39f110d1-de96-4c63-c60f-f4abe1ff12c9"
      },
      "source": [
        "nn_architecture[1]['activation']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'sigmoid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apFzoggrMYW_",
        "colab_type": "text"
      },
      "source": [
        "# Initialize Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FNVIQaL_5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(nn_architecture, initialization = \"range_initialization\", seed=5):\n",
        "\n",
        "  parameters = {}\n",
        "  num_layers = len(nn_architecture)\n",
        "\n",
        "  for i in range(1, num_layers):\n",
        "\n",
        "    if initialization == \"range_initialization\":\n",
        "      parameters['W' + str(i)] = init_layer.range_initializtion(nn_architecture[i][\"layer_size\"],\n",
        "                                            nn_architecture[i - 1][\"layer_size\"], seed)\n",
        "      # print(parameters['W1'])\n",
        "    else:\n",
        "      parameters['W' + str(i)] = init_layer.random_initialization(nn_architecture[i][\"layer_size\"],\n",
        "                                            nn_architecture[i - 1][\"layer_size\"], seed)\n",
        "\n",
        "    parameters['b' + str(i)] = np.zeros((nn_architecture[i][\"layer_size\"], 1))\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk0yEPOsggOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "58e2e579-add2-431e-cde7-e1110f8053a4"
      },
      "source": [
        "param = initialize_parameters(nn_architecture, initialization=\"range_initialization\")\n",
        "for k in param.keys():\n",
        "  print(f\"{k}: {param[k].shape}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1: (10, 900)\n",
            "b1: (10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RojXQmNkcTjY",
        "colab_type": "text"
      },
      "source": [
        "# Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg5s5MaUQKYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Z = W * X + b\n",
        "# Here A is output of previous layer\n",
        "\n",
        "def linear_forward(A_prev, W, b):\n",
        "  return np.dot(W, A_prev) + b\n",
        "\n",
        "# apply activation h:  A = h(X) \n",
        "def apply_activation(A, activation, alpha=0.01):\n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return activations.sigmoid(A)\n",
        "  elif activation == \"tanh\":\n",
        "    return activations.tanh(A)\n",
        "  elif activation == \"relu\":\n",
        "    return activations.relu(A)\n",
        "  elif activation == \"leaky_relu\":\n",
        "    return activations.leaky_relu(A, alpha)\n",
        "  elif activation == \"softmax\":\n",
        "    return loss.softmax(A)\n",
        "  elif activation == 'none':\n",
        "    #print(\"None activation used\")\n",
        "    return A\n",
        "  else:\n",
        "    print(f\"ERROR: {activation} activation not supported\")\n",
        "    sys.exit(1)\n",
        "  \n",
        "# driver forward propogation\n",
        "def model_forward(X, parameters, nn_architecture, alpha=0.01):\n",
        "\n",
        "  forward_cache = []\n",
        "  A = X\n",
        "  num_layers = len(nn_architecture)\n",
        "\n",
        "  for i in range(1, num_layers):\n",
        "    A_prev = A\n",
        "    # print(f\"{A_prev.shape}: for\")\n",
        "    W = parameters[\"W\" + str(i)]\n",
        "    b = parameters[\"b\" + str(i)]\n",
        "\n",
        "    Z = linear_forward(A_prev, W, b)\n",
        "    activation = nn_architecture[i]['activation']\n",
        "    # print(activation)\n",
        "    A = apply_activation(Z, activation, alpha)\n",
        "    # print(A)\n",
        "    forward_cache.append(((A_prev, W, b), Z))\n",
        "\n",
        "  # print(\"after loop\")\n",
        "  # print(A)\n",
        "  return A, forward_cache  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuiILZ1FMuJA",
        "colab_type": "text"
      },
      "source": [
        "# Backpropogation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaNyla5vMtrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dz, cache):\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1]\n",
        "  # print(m)\n",
        "  dw = (1 / m) * np.dot(dz, A_prev.T)\n",
        "  db = (1 / m) * np.sum(dz, axis=1, keepdims=True)\n",
        "  dA_prev = np.dot(W.T, dz)\n",
        "\n",
        "  assert dA_prev.shape == A_prev.shape\n",
        "  assert dw.shape == W.shape\n",
        "  assert db.shape == b.shape\n",
        "\n",
        "  return dA_prev, dw, db      "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSHajBeXr9fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_activation_backward(dA, cache, activation_fn):\n",
        "  linear_cache, activation_cache = cache\n",
        "\n",
        "  if activation_fn == \"sigmoid\":\n",
        "    dZ = activations.sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation_fn == \"tanh\":\n",
        "    dZ = activations.tanh_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation_fn == \"relu\":\n",
        "    dZ = activations.relu_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation_fn == \"leaky_relu\":\n",
        "    dZ = activations.leaky_relu_backward(dA, activation_cache)\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "  \n",
        "  elif activation_fn == \"none\":\n",
        "    dZ = dA\n",
        "    dA_prev, dw, db = linear_backward(dZ, linear_cache)\n",
        "  \n",
        "  else:\n",
        "    print(\"Activation not available\")\n",
        "    sys.exit(1)\n",
        "\n",
        "  return dA_prev, dw, db\n",
        "\n",
        "  \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKQVgsCqetdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_backward(AL, y, caches, nn_architecture):\n",
        "  \n",
        "  y = y.reshape(AL.shape)\n",
        "  L = len(caches)\n",
        "  # print(f\"Len of cache: {len(caches)}\")\n",
        "  grads = {}\n",
        "\n",
        "  dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
        "  # print(caches.keys())\n",
        "  # print(grads.keys())\n",
        "  # print(len(caches))\n",
        "  grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = apply_activation_backward(dAL, caches[L-1], nn_architecture[L]['activation'])\n",
        "\n",
        "  for l in range(L - 1, 0, -1):\n",
        "    # print(l)\n",
        "    current_cache = caches[l - 1]\n",
        "    grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)],  \\\n",
        "        grads[\"db\" + str(l)] = apply_activation_backward(\n",
        "            grads[\"dA\" + str(l)], current_cache, \n",
        "            nn_architecture[l]['activation']\n",
        "        )\n",
        "    \n",
        "  return grads"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIGxsD-84Nhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, lr):\n",
        "\n",
        "  L = len(parameters) // 2\n",
        "\n",
        "  for l in range(1, L + 1):\n",
        "    parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - \\\n",
        "            lr * grads[\"dW\" + str(l)]\n",
        "    parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - \\\n",
        "            lr * grads[\"db\" + str(l)]\n",
        "  return parameters"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sl_Wp-H5otj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X, y, nn_architecture, exp_name, initialization='range_initialisation', lr=0.01, num_iterations=2000, print_cost=True, checkpoint_initialisation=None):\n",
        "\n",
        "  np.random.seed(1)\n",
        "\n",
        "  if checkpoint_initialisation == None:\n",
        "    parameters = initialize_parameters(nn_architecture, initialization)\n",
        "  else:\n",
        "    print(f\"Loading checkpoints from file {checkpoint_initialisation}\")\n",
        "    parameters = np.load(checkpoint_initialisation, allow_pickle=True).item()\n",
        "\n",
        "  cost_list = []\n",
        "  val_loss = []\n",
        "  val_list = []\n",
        "  train_list = []\n",
        "  #iterate over iterations\n",
        "\n",
        "  for i in tqdm_notebook(range(0, num_iterations)):\n",
        "\n",
        "    #forward step\n",
        "    AL, caches = model_forward(X, parameters, nn_architecture)\n",
        "    AL_val, _ = model_forward(val_features, parameters, nn_architecture)\n",
        "    cost = loss.cross_entropy_loss(AL, y)\n",
        "    val_cost = loss.cross_entropy_loss(AL_val, y_val)\n",
        "    # print(cost)\n",
        "    grads = model_backward(AL, y, caches, nn_architecture)\n",
        "\n",
        "    parameters = update_parameters(parameters, grads, lr)\n",
        "\n",
        "    \n",
        "    print(f\"The cost after {i + 1} iterations is: Train: {cost: .4f}, Val: {val_cost: .4f}\")\n",
        "\n",
        "    train_acc = accuracy(X, parameters, y, nn_architecture)\n",
        "    acc = accuracy(val_features, parameters, y_val, nn_architecture)\n",
        "    print(f\"After {i + 1} iterations: Val Acc: {acc}; Train Acc: {train_acc}\")\n",
        "    val_list.append(acc)\n",
        "    train_list.append(train_acc)\n",
        "\n",
        "    \n",
        "    cost_list.append(cost)\n",
        "    val_loss.append(val_cost)\n",
        "    # if i % 50 == 0:\n",
        "    #   print(f\"Creating checkpoint for {i}th iteration\")\n",
        "    #   np.save(f'params-{i}-{initialization}.npy', parameters)\n",
        "\n",
        "    #plotting cost curve\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(cost_list, 'b', label='train_loss')\n",
        "  plt.plot(val_loss, 'r', label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(f\"Loss curve for the learning rate = {lr}\")\n",
        "  plt.savefig(f'fig1-loss-{exp_name}.jpg', bbox_inches='tight')\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(val_list, 'g', label=\"val acc\")\n",
        "  plt.ylim(0, 100)\n",
        "  plt.legend()\n",
        "  plt.savefig(f'val-acc-{exp_name}.jpg', bbox_inches='tight')\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(train_list, 'r', label =\"train acc\")\n",
        "  plt.ylim(0, 100)\n",
        "  plt.legend()\n",
        "  plt.savefig(f'train-loss-{exp_name}.jpg', bbox_inches='tight')\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.title(f\"Accuracy curve for the learning rate = {lr}\")\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM0ETWnl0-yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(X, parameters, y, nn_architecture):\n",
        "  probs, caches = model_forward(X, parameters, nn_architecture)\n",
        "  labels = np.argmax(probs, axis=0)\n",
        "  gt = np.argmax(y, axis=0)\n",
        "  accuracy = np.mean(labels==gt) * 100\n",
        "  \n",
        "  return accuracy"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Y6qD9v2sSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Todo\n",
        "\n",
        "#implement margin loss\n",
        "#implement regularisation"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wwpwhANGS1A",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31iXipAcF1jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.linalg import norm\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def dictionary_to_vector(params_dict):\n",
        "  count = 0\n",
        "\n",
        "  for key in params_dict.keys():\n",
        "    new_vector = np.reshape(params_dict[key], (-1, 1))\n",
        "\n",
        "    if count == 0:\n",
        "      theta_vector = new_vector\n",
        "    else:\n",
        "      theta_vector = np.concatenate((theta_vector, new_vector))\n",
        "    count += 1\n",
        "\n",
        "  return theta_vector\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vInG5mpFM_Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vector_to_dictionary(vector, nn_architecture):\n",
        "\n",
        "  L = len(nn_architecture)\n",
        "  parameters = {}\n",
        "  k = 0\n",
        "\n",
        "  for l in range(1, L):\n",
        "\n",
        "    w_dim = nn_architecture[l]['layer_size'] * nn_architecture[l - 1]['layer_size']\n",
        "    b_dim = nn_architecture[l]['layer_size']\n",
        "\n",
        "    temp_dim = k + w_dim\n",
        "\n",
        "    parameters[\"W\" + str(l)] = vector[k:temp_dim].reshape(nn_architecture[l]['layer_size'], nn_architecture[l - 1]['layer_size'])\n",
        "    parameters[\"b\" + str(l)] = vector[temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n",
        "\n",
        "    k += w_dim + b_dim\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTMNkz1qOQM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradients_to_vector(gradients):\n",
        "\n",
        "  valid_grads = [key for key in gradients.keys() if not key.startswith(\"dA\")]\n",
        "  L = len(valid_grads) // 2\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  for l in range(1, L + 1):\n",
        "    if count == 0:\n",
        "      new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n",
        "      new_grads = np.concatenate((new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
        "\n",
        "    else:\n",
        "      new_grads = np.concatenate((new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)))\n",
        "      new_grads = np.concatenate((new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
        "\n",
        "    count += 1\n",
        "  return new_grads"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuTgjZ6vHo7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop_cost(X, parameters, Y, nn_architecture):\n",
        "\n",
        "  AL, _ = model_forward(X, parameters, nn_architecture)\n",
        "  cost = loss.cross_entropy_loss(AL, Y)\n",
        "  # print(cost)\n",
        "  return cost\n",
        "\n",
        "def gradient_check(parameters, gradients, X, Y, nn_architecture, epsilon=1e-4):\n",
        "\n",
        "  parameters_vector = dictionary_to_vector(parameters)\n",
        "  gradients_vector = gradients_to_vector(gradients)\n",
        "\n",
        "  grads_approx = np.zeros_like(parameters_vector)\n",
        "\n",
        "  for i in range(len(parameters_vector)):\n",
        "\n",
        "    theta_plus = np.copy(parameters_vector)\n",
        "    theta_plus[i] = theta_plus[i] + epsilon\n",
        "    j_plus = forward_prop_cost(X, vector_to_dictionary(theta_plus, nn_architecture), Y, nn_architecture)\n",
        "\n",
        "\n",
        "    theta_minus = np.copy(parameters_vector)\n",
        "    theta_minus[i] = theta_minus[i] - epsilon\n",
        "    j_minus = forward_prop_cost(X, vector_to_dictionary(theta_minus, nn_architecture), Y, nn_architecture)\n",
        "\n",
        "    grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n",
        "\n",
        "    #print(f\"grads_approx{i}: {grads_approx[i]} and {gradients_vector[i]}\")\n",
        "\n",
        "  \n",
        "\n",
        "  numerator = norm(gradients_vector - grads_approx)\n",
        "  denominator = norm(grads_approx) + norm(gradients_vector)\n",
        "\n",
        "  difference = numerator / denominator\n",
        "\n",
        "  if difference > 10e-4:\n",
        "    print(f\"Backprop Wrong: difference = {difference}\")\n",
        "  else:\n",
        "    print(f\"Backprop Correct: difference = {difference}\")\n",
        "\n",
        "  return difference\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAg4HHQnnML1",
        "colab_type": "text"
      },
      "source": [
        "### Execute the following cell for gradient checking. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WH5lM5MRgu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "342876a8-b79d-464c-9b2c-bf3331be392b"
      },
      "source": [
        "# performing graident checking\n",
        "nn_architecture = [\n",
        "  {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "  {\"layer_size\": 300, \"activation\": \"relu\"},\n",
        "  {\"layer_size\": 100, \"activation\": \"relu\"},\n",
        "  {\"layer_size\": 10, \"activation\": \"sigmoid\"}\n",
        "]\n",
        "def perform_gradient_check():\n",
        "  parameters = initialize_parameters(nn_architecture)\n",
        "\n",
        "  perms = np.random.permutation(train_features.shape[1])\n",
        "  index = perms[:1]\n",
        "  print(train_features[:, index].shape)\n",
        "  print(y_train[:, index].shape)\n",
        "\n",
        "  AL, caches = model_forward(train_features[:, index], parameters, nn_architecture)\n",
        "  grads = model_backward(AL, y_train[:, index], caches, nn_architecture)\n",
        "  #print(grads)\n",
        "  difference = gradient_check(parameters, grads, train_features[:, index], y_train[:, index], nn_architecture)\n",
        "\n",
        "perform_gradient_check()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 1)\n",
            "(10, 1)\n",
            "Backprop Correct: difference = 8.085386956443845e-11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxLlpzjmnVWC",
        "colab_type": "text"
      },
      "source": [
        "### Execute the following cell for training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eCMTfBuLKwbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b60999c5a1bf4ed98bd9b9399abed60e",
            "8cda148d6eac47fb91ad69a01f990a0f",
            "1ed8eeb4361542d08bc47cba70241c84",
            "c265cae3ed924354822d728902c0b5c9",
            "94bf97fbfa264eb389d4ad7f2745c2d9",
            "9b835ee03561458687661cb250f2cbde",
            "0a8df44f379a45b3962a28cc66666a37",
            "6daaf879d1c34c919ce4aac181dab264"
          ]
        },
        "outputId": "5f0b193e-fc2e-4b4a-fdde-53b0a614eb81"
      },
      "source": [
        "# # Experiment 1: Single Layer Perceptron. range initialisatioin, 2000 iterations, relu\n",
        "# fp = open('results.txt', 'a')\n",
        "\n",
        "# params = model(train_features, y_train, nn_architecture, exp_name='Exp1', initialization='range_initialization', lr=0.1, num_iterations=1000)\n",
        "\n",
        "\n",
        "# accuracy(test_features, params, y_test, nn_architecture)\n",
        "\n",
        "# AL, _ = model_forward(test_features, params, nn_architecture)\n",
        "\n",
        "# from statistics import print_statistics\n",
        "\n",
        "# stats = print_statistics(AL, y_test)\n",
        "\n",
        "# fp.write(\"Experiment: Single Layer Perceptron. range initialisatioin, 1000 iterations, relu\\n\\n\")\n",
        "# fp.write(stats)\n",
        "# fp.close()\n",
        "\n",
        "\n",
        "# # Experiment 2: Single Layer Perceptron. random initialisatioin, 2000 iterations, relu\n",
        "# fp = open('results.txt', 'a')\n",
        "\n",
        "# params = model(train_features, y_train, nn_architecture, exp_name='Exp2', initialization='random_initialization', lr=0.1, num_iterations=1000)\n",
        "\n",
        "\n",
        "# accuracy(test_features, params, y_test, nn_architecture)\n",
        "\n",
        "# AL, _ = model_forward(test_features, params, nn_architecture)\n",
        "\n",
        "# from statistics import print_statistics\n",
        "\n",
        "# stats = print_statistics(AL, y_test)\n",
        "\n",
        "# fp.write(\"Experiment: Single Layer Perceptron. random initialisatioin, 1000 iterations, relu\\n\\n\")\n",
        "# fp.write(stats)\n",
        "# fp.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Experiment 3: Multi Layer Perceptron. range initialisatioin, 1000 iterations, relu\n",
        "# fp = open('results.txt', 'a')\n",
        "\n",
        "# nn_architecture = [\n",
        "#   {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "#   {\"layer_size\": 300, \"activation\": \"relu\"},\n",
        "#   {\"layer_size\": 100, \"activation\": \"relu\"},\n",
        "#   {\"layer_size\": 10, \"activation\": \"sigmoid\"}\n",
        "# ]\n",
        "\n",
        "# params = model(train_features, y_train, nn_architecture, exp_name='Exp3', initialization='range_initialization', lr=0.1, num_iterations=300)\n",
        "\n",
        "\n",
        "# accuracy(test_features, params, y_test, nn_architecture)\n",
        "\n",
        "# AL, _ = model_forward(test_features, params, nn_architecture)\n",
        "\n",
        "# from statistics import print_statistics\n",
        "\n",
        "# stats = print_statistics(AL, y_test)\n",
        "\n",
        "# fp.write(\"Experiment: Multi Layer Perceptron. range initialisatioin, 1000 iterations, relu\\n\\n\")\n",
        "# fp.write(stats)\n",
        "# fp.close()\n",
        "\n",
        "\n",
        "# # Experiment 4: Multi Layer Perceptron. range initialisatioin, 1000 iterations, tanh\n",
        "# fp = open('results.txt', 'a')\n",
        "\n",
        "# nn_architecture = [\n",
        "#   {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "#   {\"layer_size\": 300, \"activation\": \"tanh\"},\n",
        "#   {\"layer_size\": 100, \"activation\": \"tanh\"},\n",
        "#   {\"layer_size\": 10, \"activation\": \"sigmoid\"}\n",
        "# ]\n",
        "\n",
        "# params = model(train_features, y_train, nn_architecture, exp_name='Exp4', initialization='range_initialization', lr=0.1, num_iterations=300)\n",
        "\n",
        "\n",
        "# accuracy(test_features, params, y_test, nn_architecture)\n",
        "\n",
        "# AL, _ = model_forward(test_features, params, nn_architecture)\n",
        "\n",
        "# from statistics import print_statistics\n",
        "\n",
        "# stats = print_statistics(AL, y_test)\n",
        "\n",
        "# fp.write(\"Experiment: Multiple Layer Perceptron. range initialisatioin, 1000 iterations, tanh\\n\\n\")\n",
        "# fp.write(stats)\n",
        "# fp.close()\n",
        "\n",
        "# # Experiment 5: Multi Layer Perceptron. range initialisatioin, 1000 iterations, sigmoid\n",
        "# fp = open('results.txt', 'a')\n",
        "\n",
        "# nn_architecture = [\n",
        "#   {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "#   {\"layer_size\": 300, \"activation\": \"sigmoid\"},\n",
        "#   {\"layer_size\": 100, \"activation\": \"sigmoid\"},\n",
        "#   {\"layer_size\": 10, \"activation\": \"sigmoid\"}\n",
        "# ]\n",
        "\n",
        "\n",
        "\n",
        "# params = model(train_features, y_train, nn_architecture, exp_name='Exp5', initialization='range_initialization', lr=0.1, num_iterations=300)\n",
        "\n",
        "\n",
        "# accuracy(test_features, params, y_test, nn_architecture)\n",
        "\n",
        "# AL, _ = model_forward(test_features, params, nn_architecture)\n",
        "\n",
        "# from statistics import print_statistics\n",
        "\n",
        "# stats = print_statistics(AL, y_test)\n",
        "\n",
        "# fp.write(\"Experiment: Multiple Layer Perceptron. range initialisatioin, 1000 iterations, sigmoid\\n\\n\")\n",
        "# fp.write(stats)\n",
        "# fp.close()\n",
        "\n",
        "\n",
        "# Experiment 6: Multi Layer Perceptron. random initialisatioin, 1000 iterations, relu\n",
        "fp = open('results.txt', 'a')\n",
        "\n",
        "nn_architecture = [\n",
        "  {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "  {\"layer_size\": 300, \"activation\": \"relu\"},\n",
        "  {\"layer_size\": 100, \"activation\": \"relu\"},\n",
        "  {\"layer_size\": 10, \"activation\": \"sigmoid\"}\n",
        "]\n",
        "\n",
        "params = model(train_features, y_train, nn_architecture, exp_name='Exp6', initialization='random_initialization', lr=0.1, num_iterations=300)\n",
        "\n",
        "\n",
        "accuracy(test_features, params, y_test, nn_architecture)\n",
        "\n",
        "AL, _ = model_forward(test_features, params, nn_architecture)\n",
        "\n",
        "from statistics import print_statistics\n",
        "\n",
        "stats = print_statistics(AL, y_test)\n",
        "\n",
        "fp.write(\"Experiment: Multiple Layer Perceptron. random initialisatioin, 1000 iterations, relu\\n\\n\")\n",
        "fp.write(stats)\n",
        "fp.close()\n",
        "\n",
        "# Experiment 7: Multi Layer Perceptron. random initialisatioin, 1000 iterations, leaky relu\n",
        "fp = open('results.txt', 'a')\n",
        "\n",
        "nn_architecture = [\n",
        "  {\"layer_size\": 900, \"activation\": \"none\"},\n",
        "  {\"layer_size\": 300, \"activation\": \"leaky_relu\"},\n",
        "  {\"layer_size\": 100, \"activation\": \"leaky_relu\"},\n",
        "  {\"layer_size\": 10, \"activation\": \"sigmoid\"}\n",
        "]\n",
        "\n",
        "params = model(train_features, y_train, nn_architecture, exp_name='Exp7', initialization='range_initialization', lr=0.1, num_iterations=300)\n",
        "\n",
        "\n",
        "accuracy(test_features, params, y_test, nn_architecture)\n",
        "\n",
        "AL, _ = model_forward(test_features, params, nn_architecture)\n",
        "\n",
        "from statistics import print_statistics\n",
        "\n",
        "stats = print_statistics(AL, y_test)\n",
        "\n",
        "fp.write(\"Experiment: Multiple Layer Perceptron. random initialisatioin, 1000 iterations, relu\\n\\n\")\n",
        "fp.write(stats)\n",
        "fp.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b60999c5a1bf4ed98bd9b9399abed60e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The cost after 1 iterations is: Train:  6.9311, Val:  6.9311\n",
            "After 1 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 2 iterations is: Train:  6.7110, Val:  6.7112\n",
            "After 2 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 3 iterations is: Train:  6.5013, Val:  6.5017\n",
            "After 3 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 4 iterations is: Train:  6.3007, Val:  6.3013\n",
            "After 4 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 5 iterations is: Train:  6.1081, Val:  6.1088\n",
            "After 5 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 6 iterations is: Train:  5.9219, Val:  5.9229\n",
            "After 6 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 7 iterations is: Train:  5.7409, Val:  5.7421\n",
            "After 7 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 8 iterations is: Train:  5.5633, Val:  5.5648\n",
            "After 8 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 9 iterations is: Train:  5.3872, Val:  5.3888\n",
            "After 9 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 10 iterations is: Train:  5.2100, Val:  5.2119\n",
            "After 10 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 11 iterations is: Train:  5.0290, Val:  5.0312\n",
            "After 11 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 12 iterations is: Train:  4.8407, Val:  4.8432\n",
            "After 12 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 13 iterations is: Train:  4.6412, Val:  4.6441\n",
            "After 13 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 14 iterations is: Train:  4.4262, Val:  4.4294\n",
            "After 14 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 15 iterations is: Train:  4.1912, Val:  4.1947\n",
            "After 15 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 16 iterations is: Train:  3.9329, Val:  3.9367\n",
            "After 16 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 17 iterations is: Train:  3.6504, Val:  3.6543\n",
            "After 17 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 18 iterations is: Train:  3.3481, Val:  3.3520\n",
            "After 18 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 19 iterations is: Train:  3.0377, Val:  3.0416\n",
            "After 19 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 20 iterations is: Train:  2.7377, Val:  2.7417\n",
            "After 20 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 21 iterations is: Train:  2.4676, Val:  2.4719\n",
            "After 21 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 22 iterations is: Train:  2.2397, Val:  2.2446\n",
            "After 22 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 23 iterations is: Train:  2.0563, Val:  2.0620\n",
            "After 23 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 24 iterations is: Train:  1.9123, Val:  1.9188\n",
            "After 24 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 25 iterations is: Train:  1.7999, Val:  1.8072\n",
            "After 25 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 26 iterations is: Train:  1.7117, Val:  1.7197\n",
            "After 26 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 27 iterations is: Train:  1.6418, Val:  1.6503\n",
            "After 27 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 28 iterations is: Train:  1.5856, Val:  1.5944\n",
            "After 28 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 29 iterations is: Train:  1.5398, Val:  1.5489\n",
            "After 29 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 30 iterations is: Train:  1.5020, Val:  1.5112\n",
            "After 30 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 31 iterations is: Train:  1.4704, Val:  1.4797\n",
            "After 31 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 32 iterations is: Train:  1.4438, Val:  1.4531\n",
            "After 32 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 33 iterations is: Train:  1.4210, Val:  1.4304\n",
            "After 33 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 34 iterations is: Train:  1.4015, Val:  1.4108\n",
            "After 34 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 35 iterations is: Train:  1.3845, Val:  1.3937\n",
            "After 35 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 36 iterations is: Train:  1.3696, Val:  1.3787\n",
            "After 36 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 37 iterations is: Train:  1.3565, Val:  1.3655\n",
            "After 37 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 38 iterations is: Train:  1.3448, Val:  1.3538\n",
            "After 38 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 39 iterations is: Train:  1.3344, Val:  1.3433\n",
            "After 39 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 40 iterations is: Train:  1.3251, Val:  1.3339\n",
            "After 40 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 41 iterations is: Train:  1.3166, Val:  1.3253\n",
            "After 41 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 42 iterations is: Train:  1.3090, Val:  1.3176\n",
            "After 42 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 43 iterations is: Train:  1.3020, Val:  1.3106\n",
            "After 43 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 44 iterations is: Train:  1.2956, Val:  1.3041\n",
            "After 44 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 45 iterations is: Train:  1.2897, Val:  1.2982\n",
            "After 45 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 46 iterations is: Train:  1.2844, Val:  1.2928\n",
            "After 46 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 47 iterations is: Train:  1.2794, Val:  1.2878\n",
            "After 47 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 48 iterations is: Train:  1.2747, Val:  1.2831\n",
            "After 48 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 49 iterations is: Train:  1.2704, Val:  1.2788\n",
            "After 49 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 50 iterations is: Train:  1.2664, Val:  1.2748\n",
            "After 50 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 51 iterations is: Train:  1.2627, Val:  1.2710\n",
            "After 51 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 52 iterations is: Train:  1.2591, Val:  1.2674\n",
            "After 52 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 53 iterations is: Train:  1.2558, Val:  1.2641\n",
            "After 53 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 54 iterations is: Train:  1.2527, Val:  1.2610\n",
            "After 54 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 55 iterations is: Train:  1.2497, Val:  1.2580\n",
            "After 55 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 56 iterations is: Train:  1.2469, Val:  1.2552\n",
            "After 56 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 57 iterations is: Train:  1.2442, Val:  1.2526\n",
            "After 57 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 58 iterations is: Train:  1.2417, Val:  1.2500\n",
            "After 58 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 59 iterations is: Train:  1.2392, Val:  1.2476\n",
            "After 59 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 60 iterations is: Train:  1.2369, Val:  1.2453\n",
            "After 60 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 61 iterations is: Train:  1.2347, Val:  1.2431\n",
            "After 61 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 62 iterations is: Train:  1.2326, Val:  1.2410\n",
            "After 62 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 63 iterations is: Train:  1.2305, Val:  1.2390\n",
            "After 63 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 64 iterations is: Train:  1.2285, Val:  1.2371\n",
            "After 64 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 65 iterations is: Train:  1.2266, Val:  1.2352\n",
            "After 65 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 66 iterations is: Train:  1.2248, Val:  1.2334\n",
            "After 66 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 67 iterations is: Train:  1.2230, Val:  1.2317\n",
            "After 67 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 68 iterations is: Train:  1.2213, Val:  1.2300\n",
            "After 68 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 69 iterations is: Train:  1.2197, Val:  1.2284\n",
            "After 69 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 70 iterations is: Train:  1.2180, Val:  1.2268\n",
            "After 70 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 71 iterations is: Train:  1.2165, Val:  1.2253\n",
            "After 71 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n",
            "The cost after 72 iterations is: Train:  1.2150, Val:  1.2238\n",
            "After 72 iterations: Val Acc: 82.08697675320394; Train Acc: 82.29373405160193\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}